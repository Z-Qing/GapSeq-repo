import pandas as pd
from picasso.gausslq import locs_from_fits_gpufit, fit_spots_gpufit, fit_spots_parallel, locs_from_fits
from picasso.io import load_movie
from picasso.localize import identify_async, identifications_from_futures, get_spots
from picasso.aim import aim
from picasso.clusterer import dbscan
from picasso.postprocess import link
from picasso.lib import ensure_sanity
import cupy as cp
from cupyx.scipy.ndimage import shift as cupy_shift
from scipy.spatial import KDTree
from scipy.ndimage import uniform_filter, shift
import numpy as np
from numpy.lib import recfunctions as rfn
import ruptures as rpt
import multiprocessing
import matplotlib.pyplot as plt



class one_channel_movie(object):
    def __init__(self, movie_path, roi=None, frame_range=None):
        self.movie_path = movie_path
        self.roi = roi
        self.frame_range = frame_range
        self.camera_info = {}

        self.movie = None
        self.locs = None
        self.info = None
        self.cluster_param = None

    def __getitem__(self, index):
        return self.movie[index]

    # this function is modified from the localize function in picasso.localize
    # and it exports the fitting quality as well
    def movie_format(self, baseline=400):
        movie, info = load_movie(self.movie_path)

        self.camera_info["Baseline"] = baseline
        self.camera_info["Sensitivity"] = 2.5
        self.camera_info["Gain"] = 1
        self.camera_info["qe"] = 0.82

        # handle the roi before feeding it into the picasso functions
        if self.roi is not None:
            movie = movie[:, self.roi[0]: self.roi[2], self.roi[1]: self.roi[3]]

            info[0]['Width'] = self.roi[3] - self.roi[1]
            info[0]['Height'] = self.roi[2] - self.roi[0]

        # change the frame range if it is not None
        if self.frame_range is not None:
            if isinstance(self.frame_range, (list, tuple)):
                movie = movie[self.frame_range[0]: self.frame_range[1], :, :]
                info[0]['Frames'] = self.frame_range[1] - self.frame_range[0]

            if isinstance(self.frame_range, int):
                movie = movie[self.frame_range, :, :]
                movie = movie[np.newaxis, :, :]
                info[0]['Frames'] = 1

            else:
                raise ValueError("frame_range must be a list/tuple for a range or"
                                 " an integer for a single frame")

        self.movie = movie
        self.info = info

        return



    # this function selects  and modifies a part of codes frm picasso.main
    def lq_gpu_fitting(self, min_net_gradient, box):
        camera_info = self.camera_info

        current, futures = identify_async(self.movie, min_net_gradient, box, roi=None)
        ids = identifications_from_futures(futures)

        spots = get_spots(self.movie, ids, box, camera_info)
        theta = fit_spots_gpufit(spots)
        em = camera_info["Gain"] > 1
        locs = locs_from_fits_gpufit(ids, theta, box, em)

        self.locs = locs

        return

    def lq_cpu_fitting(self, min_net_gradient, box):
        camera_info = self.camera_info

        current, futures = identify_async(self.movie, min_net_gradient, box, roi=None)
        ids = identifications_from_futures(futures)

        spots = get_spots(self.movie, ids, box, camera_info)
        theta = fit_spots_parallel(spots)
        locs = locs_from_fits(ids, theta, box, camera_info["Gain"])

        self.locs = locs

        return

    def lq_fitting(self, GPU, min_net_gradient=400, box=5):
        if self.movie is None or self.info is None:
            self.movie_format()

        if GPU:
            self.lq_gpu_fitting(min_net_gradient, box)
        else:
            self.lq_cpu_fitting(min_net_gradient, box)

        localize_info = {
            "Generated by": "Picasso Localize",
            "ROI": None,
            "Box Size": box,
            "Min. Net Gradient": min_net_gradient,
            "Convergence Criterion": 0,
            "Max. Iterations": 0,
            "Pixelsize": 117,
            "Fit method": 'lq'}

        self.info.append(localize_info)

        self.locs = ensure_sanity(self.locs, self.info)

        return


    def drift_correction(self, GPU=True, drift=None, segmentation=100, intersect_d=20 / 117, roi_r=60 / 117):
        # self.lq_gpu_fitting(min_net_gradient=min_net_gradient, box=box)
        # the movie will be set during the lq_gpu_fitting
        if drift is None:
            if max(self.locs['frame']) >= 3 * segmentation:
                corrected_locs, new_info, drift = aim(self.locs, self.info,
                            segmentation=segmentation, intersect_d=intersect_d, roi_r=roi_r)

                drift = drift.view(np.recarray)
                self.locs = corrected_locs
                self.info = new_info

            else:
                drift = np.zeros((len(self.locs), 2))


        if GPU:
                # apply negative drift frame by frame using GPU
            movie_gpu = cp.asarray(self.movie)
            corrected_movie_gpu = cp.empty_like(movie_gpu)
            for i in range(movie_gpu.shape[0]):
                aim_shift = (-drift[i][1], -drift[i][0])
                corrected_movie_gpu[i] = cupy_shift(movie_gpu[i], aim_shift, mode='constant', cval=0, order=0)

            corrected_movie = cp.asnumpy(corrected_movie_gpu)

        else:
            corrected_movie = np.empty_like(self.movie)
            for i in range(self.movie.shape[0]):
                aim_shift = (-drift[i][1], -drift[i][0])
                corrected_movie[i] = shift(self.movie[i], aim_shift, mode='constant', cval=0, order=0)

        self.movie = corrected_movie

        return drift



    def dbscan(self, eps=0.5, min_samples=20):
        if self.locs is None:
            raise ValueError('Please run localization function first')

        clusters = dbscan(self.locs, radius=eps, min_samples=min_samples)

        self.locs = clusters

        self.cluster_param = pd.DataFrame(self.locs).groupby('group').mean()[['x', 'y']]
        self.cluster_param['count'] = pd.DataFrame(self.locs).groupby('group').size()

        return


    def link(self, r_max=1, max_dark_time=1):
        if self.locs is None:
            raise ValueError('Please run localization function first')

        linked_locs = link(self.locs, self.info, r_max=r_max, max_dark_time=max_dark_time)

        self.locs = linked_locs


        return


    def find_no_neighbour_mask(self, locs, box_radius):
        points = np.column_stack((locs['x'], locs['y']))

        # Build KDTree for fast neighbor lookup
        tree = KDTree(points)

        # Find neighbors within the given radius
        indices = tree.query_ball_point(points, box_radius)

        # Keep only points with no close neighbors (excluding self)
        keep_mask = np.array([len(neigh) == 1 for neigh in indices])

        return keep_mask


    def overlap_prevent(self, box_radius=2):
        # ----------------- remove overlapping locs frame by frame -----------------

        unique_frames = np.unique(self.locs['frame'])  # Get all unique frame IDs
        filtered_locs = []  # Store results for all frames

        for frame in unique_frames:
            # Select points in the current frame
            mask = self.locs['frame'] == frame
            frame_locs = self.locs[mask]

            if len(frame_locs) == 0:
                continue  # Skip empty frames

            keep_mask = self.find_no_neighbour_mask(frame_locs, box_radius)
            filtered_locs.append(frame_locs[keep_mask])

        concatenated_locs = rfn.stack_arrays(filtered_locs, asrecarray=True)
        self.locs = concatenated_locs.view(np.recarray)

        return


    @staticmethod
    def get_binding_event_num(trace, index, penalty=5, min_size=10, min_intensity_increase=100,
                              display=False):
        algo = rpt.KernelCPD(kernel='rbf', min_size=min_size).fit(trace)
        bkps = algo.predict(pen=penalty)
        bkps = np.insert(bkps, 0, 0)

        starts = bkps[:-1]
        ends = bkps[1:]

        intensities = np.array([np.mean(trace[start + 1: end - 1]) for start, end in zip(starts, ends)])


        if display:
            plt.plot(np.arange(len(trace)), trace)
            plt.vlines(bkps, ymin=min(trace), ymax=max(trace), colors='black', linestyles='dashed')
            plt.title(index)
            print(intensities)
            print(np.sum(intensities > min_intensity_increase))
            plt.show()

        binding_event_num = np.sum(intensities > min_intensity_increase)

        return binding_event_num, index





    def trace_analysis(self, box_size=3, display=False):

        if self.cluster_param is None:
            raise ValueError('Please run dbscan first')
        else:
            pos = np.round(self.cluster_param[['x', 'y']]).astype(int).to_numpy()

        # extract intensity traces
        smoothed = uniform_filter(self.movie, size=(0, box_size, box_size), mode='nearest')

        pos_indices = []
        intensities = np.zeros((len(pos), self.movie.shape[0]))
        for i, (x, y) in enumerate(pos):
            intensities[i, :] = smoothed[:, y, x]
            pos_indices.append(i)

        # change point detection

        index_eventNum = []
        if display:
            random_ind_list = np.arange(len(pos))
            np.random.shuffle(random_ind_list)
            for i in random_ind_list:
                even_num, i = self.get_binding_event_num(intensities[i, :], pos_indices[i], display=display)
                index_eventNum.append([i, even_num])

        else:
            data_with_indices = zip(intensities, pos_indices)
            with multiprocessing.Pool() as pool:
                for event_num, i in pool.starmap(self.get_binding_event_num, data_with_indices):
                     index_eventNum.append([i, event_num])

        index_eventNum = np.array(index_eventNum)
        self.cluster_param.loc[index_eventNum[:, 0], 'event_num'] = index_eventNum[:, 1]


        return

