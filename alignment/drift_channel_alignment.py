import numpy as np
from picasso.gausslq import locs_from_fits_gpufit, fit_spots_gpufit, fit_spots_parallel, locs_from_fits
from picasso.io import load_movie
from picasso.localize import identify_async, identifications_from_futures, get_spots
from picasso.aim import aim
from picasso.imageprocess import rcc
import picasso.render as _render
from picasso.clusterer import dbscan
from picasso.postprocess import link
from tifffile import imwrite, imread
from concurrent.futures import ThreadPoolExecutor
import multiprocessing
from pystackreg import StackReg
from pystackreg.util import to_uint16
from scipy.ndimage import shift
import cupy as cp
from cupyx.scipy.ndimage import shift as cupy_shift


class one_channel_movie(object):
    def __init__(self, movie_path, roi=None, frame_range=None):
        self.movie_path = movie_path
        self.roi = roi
        self.frame_range = frame_range

        self.movie = None
        self.locs = None
        self.info = None


    def __getitem__(self, index):
        return self.movie[index]

    # this function is modified from the localize function in picasso.localize
    # and it exports the fitting quality as well
    def io_movie_format(self):
        camera_info = {}
        camera_info["Baseline"] = 400
        camera_info["Sensitivity"] = 2.5
        camera_info["Gain"] = 1
        camera_info["qe"] = 0.82

        movie, info = load_movie(self.movie_path)
        # handle the roi before feeding it into the picasso functions
        if self.roi is not None:
            movie = movie[:, self.roi[0]: self.roi[2], self.roi[1]: self.roi[3]]

            info[0]['Width'] = self.roi[3] - self.roi[1]
            info[0]['Height'] = self.roi[2] - self.roi[0]

        # change the frame range if it is not None
        if self.frame_range is not None:
            if isinstance(self.frame_range, (list, tuple)):
                movie = movie[self.frame_range[0]: self.frame_range[1], :, :]
                info[0]['Frames'] = self.frame_range[1] - self.frame_range[0]

            if isinstance(self.frame_range, int):
                movie = movie[self.frame_range, :, :]
                movie = movie[np.newaxis, :, :]
                info[0]['Frames'] = 1

            else:
                raise ValueError("frame_range must be a list/tuple for a range or"
                                 " an integer for a single frame")

        return movie, camera_info, info


    # this function selects  and modifies a part of codes frm picasso.main
    def lq_gpu_fitting(self, min_net_gradient=400, box=5):
        movie, camera_info, info = self.io_movie_format()

        current, futures = identify_async(movie, min_net_gradient, box, roi=None)
        ids = identifications_from_futures(futures)

        spots = get_spots(movie, ids, box, camera_info)
        theta = fit_spots_gpufit(spots)
        em = camera_info["Gain"] > 1
        locs = locs_from_fits_gpufit(ids, theta, box, em)

        localize_info = {
            "Generated by": "Picasso Localize",
            "ROI": None,
            "Box Size": box,
            "Min. Net Gradient": min_net_gradient,
            "Convergence Criterion": 0,
            "Max. Iterations": 0,
            "Pixelsize": 117,
            "Fit method": 'lq-gpu'}

        info.append(localize_info)
        self.locs = locs
        self.info = info
        self.movie = movie

        return



    def lq_cpu_fitting(self, min_net_gradient=400, box=5):
        movie, camera_info, info = self.io_movie_format()

        current, futures = identify_async(movie, min_net_gradient, box, roi=None)
        ids = identifications_from_futures(futures)

        spots = get_spots(movie, ids, box, camera_info)
        theta = fit_spots_parallel(spots)
        locs = locs_from_fits(ids, theta, box, camera_info["Gain"])

        localize_info = {
            "Generated by": "Picasso Localize",
            "ROI": None,
            "Box Size": box,
            "Min. Net Gradient": min_net_gradient,
            "Convergence Criterion": 0,
            "Max. Iterations": 0,
            "Pixelsize": 117,
            "Fit method": 'lq'}

        info.append(localize_info)

        self.locs = locs
        self.info = info
        self.movie = movie

        return


    def lq_fitting(self, GPU, min_net_gradient=400, box=5):
        if GPU:
            self.lq_gpu_fitting(min_net_gradient, box)
        else:
            self.lq_cpu_fitting(min_net_gradient, box)


    def drift_correction(self, GPU=True, segmentation=100, intersect_d=20 / 117, roi_r=60 / 117):
        #self.lq_gpu_fitting(min_net_gradient=min_net_gradient, box=box)
        # the movie will be set during the lq_gpu_fitting
        if self.locs is None:
            raise ValueError("Please identify locs form the movie first")

        if max(self.locs['frame']) >= 3 * segmentation:
            corrected_locs, new_info, drift = aim(self.locs, self.info,
                                                  segmentation=segmentation, intersect_d=intersect_d, roi_r=roi_r)
            drift = drift.view(np.recarray)

            if GPU:
                # apply negative drift frame by frame using GPU
                movie_gpu = cp.asarray(self.movie)
                corrected_movie_gpu = cp.empty_like(movie_gpu)
                for i in range(movie_gpu.shape[0]):
                    aim_shift = (-drift[i][1], -drift[i][0])
                    corrected_movie_gpu[i] = cupy_shift(movie_gpu[i], aim_shift, mode='constant', cval=0, order=0)

                corrected_movie = cp.asnumpy(corrected_movie_gpu)

            else:
                corrected_movie = np.empty_like(self.movie)
                for i in range(self.movie.shape[0]):
                    aim_shift = (-drift[i][1], -drift[i][0])
                    corrected_movie[i] = shift(self.movie[i], aim_shift, mode='constant', cval=0, order=0)

            self.movie = corrected_movie
            self.locs = corrected_locs
            self.info = new_info

        else:
            raise Warning('short movie with less than {} frames, no drift correction applied'.format(3*segmentation))

        return



    def dbscan(self,  eps=0.5, min_samples=20):
        if self.locs is None:
            raise ValueError('Please run localization function first')

        clusters = dbscan(self.locs, radius=eps, min_samples=min_samples)

        self.locs = clusters

        return


    def link(self, r_max=1, max_dark_time=1):
        if self.locs is None:
            raise ValueError('Please run localization function first')

        linked_locs = link(self.locs, self.info, r_max=r_max, max_dark_time=max_dark_time)

        self.locs = linked_locs

        return

    # def gaussian_bg_remove(self, sigma_spatial=20):
    #     bg = gaussian_filter1d(self.movie, sigma=sigma_spatial, axis=1)
    #     bg = gaussian_filter1d(bg, sigma=sigma_spatial, axis=2)
    #     bg = np.where(self.movie > bg, bg, self.movie)
    #
    #     bg_removed = self.movie - bg
    #
    #     self.movie = bg_removed
    #
    #     return




def prepare_two_channel_movie(movie_path, gradient_1=400, gradient_2=400, box_1=5, box_2=5,
                              roi=None, gpu=True):
    movie = imread(movie_path)
    h, w = movie.shape[1], movie.shape[2]

    if roi is None:
        rio_1 = [0, 0, h, w//2]
        rio_2 = [0, w//2, h, w]

    else:
        rio_1 = roi[0]
        rio_2 = roi[1]

    channel_1 = one_channel_movie(movie_path, roi=rio_1)
    channel_2 = one_channel_movie(movie_path, roi=rio_2)

    if gpu:
        channel_1.lq_gpu_fitting(gradient_1, box_1)
        channel_2.lq_gpu_fitting(gradient_2, box_2)

    else:
        channel_1.lq_cpu_fitting(gradient_1, box_1)
        channel_2.lq_cpu_fitting(gradient_2, box_2)


    if channel_1.info[0]['Frames'] >= 200:
        channel_1.drift_correction(gpu)
        channel_2.drift_correction(gpu)

    # channel_1.link()
    # channel_2.link()


    return channel_1, channel_2



def process_frame(frame, transform_mat, sr):
    """
    Process a single frame (CPU-bound task).
    """
    if np.any(frame):
        return to_uint16(sr.transform(frame, tmat=transform_mat))
    else:
        return frame

def process_frame_chunk(frame_chunk, transform_mat, sr):
    """
    Process a chunk of frames using multithreading.
    """
    with ThreadPoolExecutor() as thread_pool:
        # Process all frames in the chunk in parallel using threads
        results = list(thread_pool.map(
            lambda frame: process_frame(frame, transform_mat, sr),
            frame_chunk
        ))
    return results

def stackreg_channel_alignment(mov, ref=None, transfer_matrix=None, num_processes=None):
    """
    Align all frames in `mov` using hybrid parallelism.
    """
    sr = StackReg(StackReg.RIGID_BODY)

    if transfer_matrix is None:
        mov_image = mov[0, :, :]
        transform_mat = sr.register(ref, mov_image)
    else:
        transform_mat = transfer_matrix

    # Split the frames into chunks for multiprocessing
    num_frames = mov.shape[0]
    num_processes = num_processes or multiprocessing.cpu_count()
    chunk_size = num_frames // num_processes
    frame_chunks = [mov[i:i + chunk_size] for i in range(0, num_frames, chunk_size)]

    # Use multiprocessing to process chunks in parallel
    with multiprocessing.Pool(processes=num_processes) as pool:
        chunk_results = pool.starmap(
            process_frame_chunk,
            [(chunk, transform_mat, sr) for chunk in frame_chunks]
        )

    # Combine the results from all chunks
    aligned_mov = np.concatenate(chunk_results, axis=0)

    return aligned_mov




def position_correction_locs(movie_path_list, ref_movie_path, roi=None, ref_roi=None,
                        ref_transcription=False, gpu=True,
                        save=True, channel_align_method='RCC'):

    ''' This function contains two steps: 1.acquire localizations from the movie and using aim to calculate the
        drift then apply it to the movie for drift correction 2. acquire the transformation using the average image from the
        movies then apply the same matrix for all frames in movies for channel alignment '''
# ------------------- RCC channel alignment -------------------
    if channel_align_method == 'RCC':
        # ----------- handel reference movie ---------------------
        if ref_transcription:
            channel_1, channel_2 = prepare_two_channel_movie(ref_movie_path, roi=ref_roi, gpu=gpu)

            # align the two channels
            _, ref_image_1 = _render.render(channel_1.locs, channel_1.info)
            _, ref_image_2 = _render.render(channel_2.locs, channel_2.info)

            shift_y, shift_x = rcc([ref_image_1, ref_image_2])
            aligned_movie_2 = shift(channel_2.movie, (0, -shift_y[1], -shift_x[1]), mode='constant', cval=0, order=0)

            # concatenate the two channels
            concat_ref = np.concatenate((channel_1.movie, aligned_movie_2), axis=2)
            ref = channel_1

            if save:
                imwrite(ref_movie_path.replace('.tif', '_corrected.tif'), concat_ref)

        else:
            #  regular one channel movie
            ref = one_channel_movie(ref_movie_path, ref_roi)
            ref.lq_fitting(gpu)

            if ref.info[0]['Frames'] >= 200:
                ref.drift_correction(gpu)

            if save:
                imwrite(ref_movie_path.replace('.tif', '_chopped.tif'), ref.movie)

    # ----------- handle the smlm movies ---------------------------
        for movie_path in movie_path_list:
            mov = one_channel_movie(movie_path, roi)
            mov.lq_fitting(gpu)
            mov.drift_correction(gpu)

            _, ref_image = _render.render(ref.locs, ref.info)
            _, mov_image = _render.render(mov.locs, mov.info)
            shift_y, shift_x = rcc([ref_image, mov_image])
            aligned_movie = shift(mov.movie, (0, -shift_y[1], -shift_x[1]), mode='constant', cval=0, order=0)

            if save:
                imwrite(movie_path.replace('.tif', '_corrected.tif'), aligned_movie)


# ------------------- Rigid body transformation using StackReg -------------------
    elif channel_align_method == 'StackReg':
        if ref_transcription:
            channel_1, channel_2 = prepare_two_channel_movie(ref_movie_path, roi=ref_roi)

            channel_1_avg = channel_1.movie[0, :, :]

            aligned_channel_2 = stackreg_channel_alignment(mov=channel_2.movie, ref=channel_1_avg)

            # concatenate the two channels
            concat_ref = np.concatenate((channel_1.movie, aligned_channel_2), axis=2)
            ref = channel_1

            if save:
                imwrite(ref_movie_path.replace('.tif', '_corrected.tif'), concat_ref)

        else:
            ref = one_channel_movie(ref_movie_path, ref_roi)
            if ref.info[0]['Frame'] >= 200:
                ref.lq_gpu_fitting(gpu)
                ref.drift_correction(gpu)

            if save:
                imwrite(ref_movie_path.replace('.tif', '_chopped.tif'), ref.movie)

        for movie_path in movie_path_list:
            mov = one_channel_movie(movie_path, roi)
            mov.lq_fitting(gpu)
            mov.drift_correction(gpu)

            ref_avg = ref.movie.mean(axis=0)
            aligned_movie = stackreg_channel_alignment(mov=mov.movie, ref=ref_avg)

            if save:
                imwrite(movie_path.replace('.tif', '_corrected.tif'),aligned_movie)

    else:
        raise ValueError('The channel alignment method is either RCC or StackReg')

    return




def position_correction_fiducial(movie_path_list, ref_movie_path, roi=None, ref_roi=None,
                                 gpu=True, save=True, channel_align_method='RCC'):
    ''' This function do alignment between different green channels using fiducial markers.
    And the fiducial markers can be detected easily in green channel but not red channel.
    The locs in green and red channels of transcription movie should be co-localized. Thus,
    the transformation matrix between green and red channel are acquired from there. '''

    channel_1, channel_2 = prepare_two_channel_movie(ref_movie_path, roi=ref_roi, gpu=gpu)

    _, image_1 = _render.render(channel_1.locs, channel_1.info)
    _, image_2 = _render.render(channel_2.locs, channel_2.info)

    if channel_align_method == 'RCC':
        shift_y, shift_x = rcc([image_1, image_2])
        red_to_green_transform_mat = (0, -shift_y[1], -shift_x[1])
        aligned_channel_2_movie = shift(channel_2.movie, red_to_green_transform_mat, mode='constant', cval=0, order=0)

    elif channel_align_method == 'StackReg':
        sr = StackReg(StackReg.RIGID_BODY)
        red_to_green_transform_mat = sr.register(image_1, image_2)
        aligned_channel_2_movie = stackreg_channel_alignment(mov=channel_2.movie, transfer_matrix=red_to_green_transform_mat)

    # Note the types of red_to_green_transform_mat are different with two method

    else:
        raise ValueError('channel_align_method is either RCC or StackReg')

    aligned_ref = np.concatenate((channel_1.movie, aligned_channel_2_movie), axis=2)

    if save:
        imwrite(ref_movie_path.replace('.tif', '_corrected.tif'), aligned_ref)

    # use the first frame of the reference movie as the reference image
    # the fiducial markers will stand out in the green channel
    green_ref_image = channel_1.movie[0, :, :]

    # identify locs belonging to fiducial markers
    for movie_path in movie_path_list:
        green, red = prepare_two_channel_movie(movie_path, gpu=gpu, gradient_1=5000, gradient_2=400,
                                               box_1=5, box_2=5, roi=roi)
        green_image = green.movie[0, :, :]

        if channel_align_method == 'RCC':
            # align red to green
            red_aligned_movie = shift(red.movie, red_to_green_transform_mat, mode='constant', cval=0, order=0)

            # align green to green_ref
            shift_y, shift_x = rcc([green_ref_image, green_image])
            green_to_green_ref_mat = (0, -shift_y[1], -shift_x[1])
            green_aligned_movie = shift(green.movie, green_to_green_ref_mat, mode='constant', cval=0, order=0)

            # align red to green_ref
            red_aligned_movie = shift(red_aligned_movie, green_to_green_ref_mat, mode='constant', cval=0, order=0)

        else:
            # align red to green
            red_aligned_movie = stackreg_channel_alignment(mov=red.movie, transfer_matrix=red_to_green_transform_mat)

            # align green to green_ref
            sr = StackReg(StackReg.RIGID_BODY)
            green_to_green_ref_mat = sr.register(green_ref_image, green_image)
            green_aligned_movie = stackreg_channel_alignment(mov=green.movie, transfer_matrix=green_to_green_ref_mat)

            # align red to green_ref
            red_aligned_movie = stackreg_channel_alignment(mov=red_aligned_movie, transfer_matrix=green_to_green_ref_mat)

        # concat two channels
        aligned_movie = np.concatenate((green_aligned_movie, red_aligned_movie), axis=2)

        if save:
            imwrite(movie_path.replace('.tif', '_corrected.tif'), aligned_movie)


    return


import os
def file_sort(dir_path):
    file_list = os.listdir(dir_path)
    file_list = [x for x in file_list if x.endswith('.tif')]
    trans_mov = [x for x in file_list if 'ALEX' in x]

    if len(trans_mov) != 1:
        raise ValueError('There should be only one transcription movie in the folder')
    trans_mov = trans_mov[0]


    file_list.remove(trans_mov)

    trans_mov = dir_path + '/' + trans_mov
    file_list = [dir_path + '/' + x for x in file_list]


    return trans_mov, file_list


if __name__ == "__main__":

    # ref = "H:/jagadish_data/transcription-GAPseq/IPE_trans_NTP200_degenAtto647Nexp7_IPE_trans_ALEX.tif"
    # #ref_roi = [0, 0, 684, 420]  # green channel # Note that two NIM have different width
    #
    # mov_list = ["H:/jagadish_data/transcription-GAPseq/IPE_trans_NTP200_degenAtto647Nexp7_IPE_degen100nM_S4A5D300nM-2.tif",
    #         "H:/jagadish_data/transcription-GAPseq/IPE_trans_NTP200_degenAtto647Nexp7_IPE_degen100nM_S4C5D300nM.tif",
    #         "H:/jagadish_data/transcription-GAPseq/IPE_trans_NTP200_degenAtto647Nexp7_IPE_degen100nM_S4D5A300nM.tif",
    #         "H:/jagadish_data/transcription-GAPseq/IPE_trans_NTP200_degenAtto647Nexp7_IPE_degen100nM_S4D5C300nM.tif",
    #         "H:/jagadish_data/transcription-GAPseq/IPE_trans_NTP200_degenAtto647Nexp7_IPE_degen100nM_S4D5G300nM.tif",
    #         "H:/jagadish_data/transcription-GAPseq/IPE_trans_NTP200_degenAtto647Nexp7_IPE_degen100nM_S4D5T300nM.tif",
    #         "H:/jagadish_data/transcription-GAPseq/IPE_trans_NTP200_degenAtto647Nexp7_IPE_degen100nM_S4G5D300nM.tif",
    #         "H:/jagadish_data/transcription-GAPseq/IPE_trans_NTP200_degenAtto647Nexp7_IPE_degen100nM_S4T5D300nM.tif"]
    # #roi = [0, 428, 684, 856]  # red channel
    #
    # # position_correction_locs(mov_list, ref, roi=roi, ref_roi=None, save=False, gpu=True,
    # #                     channel_align_method='StackReg', ref_transcription=True)

    folders = os.listdir('G:')
    for f in folders:
        print(f)
        ref, mov_list = file_sort('G:/' + f + '/movie')

        position_correction_fiducial(mov_list, ref, roi=None, ref_roi=None,
                                     gpu=True, save=True, channel_align_method='StackReg')






