import numpy as np
from numpy.lib import recfunctions as rfn
import pandas as pd
from picasso.gausslq import locs_from_fits_gpufit, fit_spots_gpufit, fit_spots_parallel, locs_from_fits
from picasso.io import load_movie, save_locs
from picasso.localize import identify_async, identifications_from_futures, get_spots
from picasso.aim import aim
from picasso.imageprocess import rcc
import picasso.render as _render
import re
import os
from sklearn.neighbors import NearestNeighbors


class MovieClass(object):
    def __init__(self, movie_path, roi=None, frame_range=None):
        self.movie_path = movie_path
        self.roi = roi
        self.frame_range = frame_range

        self.movie = None
        self.locs = None
        self.info = None


    def __getitem__(self, index):
        if isinstance(index, str):
            # Handle single field access like movie['x']
            return self.locs[index]
        elif isinstance(index, (list, tuple)):
            # Handle multiple field access like movie[['x', 'y']]
            return self.locs[index]
        else:
            raise ValueError("Index must be a string or a list of strings for np.recarray")


    def __getattr__(self, name):
        # If the attribute name exists in locs, return the corresponding field
        if name in self.locs.dtype.names:
            return self.locs[name]
        else:
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")


    def io_movie_format(self):
        camera_info = {}
        camera_info["Baseline"] = 400
        camera_info["Sensitivity"] = 2.5
        camera_info["Gain"] = 1
        camera_info["qe"] = 0.82

        movie, info = load_movie(self.movie_path)
        # handle the roi before feeding it into the picasso functions
        if self.roi is not None:
            movie = movie[:, self.roi[0]: self.roi[2], self.roi[1]: self.roi[3]]

            info[0]['Width'] = self.roi[3] - self.roi[1]
            info[0]['Height'] = self.roi[2] - self.roi[0]

        # change the frame range if it is not None
        if self.frame_range is not None:
            if isinstance(self.frame_range, (list, tuple)):
                movie = movie[self.frame_range[0]: self.frame_range[1], :, :]
                info[0]['Frame'] = self.frame_range[1] - self.frame_range[0]

            if isinstance(self.frame_range, int):
                movie = movie[self.frame_range, :, :]
                movie = movie[np.newaxis, :, :]
                info[0]['Frame'] = 1

            else:
                raise ValueError("frame_range must be a list/tuple for a range or"
                                 " an integer for a single frame")

        return movie, camera_info, info


    # this function selects  and modifies a part of codes frm picasso.main
    def lq_gpu_fitting(self, min_net_gradient=400, box=5):
        movie, camera_info, info = self.io_movie_format()

        current, futures = identify_async(movie, min_net_gradient, box, roi=None)
        ids = identifications_from_futures(futures)

        spots = get_spots(movie, ids, box, camera_info)
        theta = fit_spots_gpufit(spots)
        em = camera_info["Gain"] > 1
        locs = locs_from_fits_gpufit(ids, theta, box, em)

        localize_info = {
            "Generated by": "Picasso Localize",
            "ROI": None,
            "Box Size": box,
            "Min. Net Gradient": min_net_gradient,
            "Convergence Criterion": 0,
            "Max. Iterations": 0,
            "Pixelsize": 117,
            "Fit method": 'lq-gpu'}

        info.append(localize_info)
        self.locs = locs
        self.info = info

        return locs, info



    def lq_cpu_fitting(self, min_net_gradient=400, box=5):
        movie, camera_info, info = self.io_movie_format()

        current, futures = identify_async(movie, min_net_gradient, box, roi=None)
        ids = identifications_from_futures(futures)

        spots = get_spots(movie, ids, box, camera_info)
        theta = fit_spots_parallel(spots)
        locs = locs_from_fits(ids, theta, box, camera_info["Gain"])

        localize_info = {
            "Generated by": "Picasso Localize",
            "ROI": None,
            "Box Size": box,
            "Min. Net Gradient": min_net_gradient,
            "Convergence Criterion": 0,
            "Max. Iterations": 0,
            "Pixelsize": 117,
            "Fit method": 'lq'}

        info.append(localize_info)

        self.locs = locs
        self.info = info

        return


    def drift_correction(self, segmentation=100, intersect_d=20 / 117, roi_r=60 / 117):
        # parameter in the unit of pixels
        if self.locs is None:
            raise ValueError("Please identify locs form the movie first")

        corrected_locs, new_info, drift = aim(self.locs, self.info,
                                              segmentation=segmentation, intersect_d=intersect_d,
                                              roi_r=roi_r)
        drift = drift.view(np.recarray)

        self.locs = corrected_locs
        self.info = new_info

        return drift


    def find_no_neighbour_mask(self, locs, box_radius):
        points = np.column_stack((locs['x'], locs['y']))

        # Find neighbors
        nbrs = NearestNeighbors(radius=box_radius, algorithm='ball_tree').fit(points)
        adjacency_matrix = nbrs.radius_neighbors_graph(points, mode='connectivity').toarray()

        # Keep only points with no close neighbors
        keep_mask = adjacency_matrix.sum(axis=1) == 1

        return keep_mask


    def overlap_prevent(self, box_radius=2):
        # ----------------- remove overlapping locs frame by frame -----------------

        unique_frames = np.unique(self.locs['frame'])  # Get all unique frame IDs
        filtered_locs = []  # Store results for all frames

        for frame in unique_frames:
            # Select points in the current frame
            mask = self.locs['frame'] == frame
            frame_locs = self.locs[mask]

            if len(frame_locs) == 0:
                continue  # Skip empty frames

            keep_mask = self.find_no_neighbour_mask(frame_locs, box_radius)
            filtered_locs.append(frame_locs[np.where(keep_mask)])

        concatenated_locs = rfn.stack_arrays(filtered_locs, asrecarray=True)
        self.locs = concatenated_locs.view(np.recarray)

        return


def neighbour_counting(ref_points, mov_points, nuc, box_radius=2):
    ref_points = np.column_stack((ref_points['x'], ref_points['y']))
    mov_points = np.column_stack((mov_points['x'], mov_points['y']))

    nbrs = NearestNeighbors(radius=box_radius)
    nbrs.fit(mov_points)

    # Find neighbors for all reference points at once
    indices = nbrs.radius_neighbors(ref_points, return_distance=False)

    params = []
    for i, (ref_point, neighbor_indices) in enumerate(zip(ref_points, indices)):
        neighbor_count = len(mov_points[neighbor_indices]) if len(neighbor_indices) > 0 else 0
        params.append([neighbor_count])

    params = pd.DataFrame(params, columns=['{}'.format(nuc)])
    params.index.name = 'ref_index'

    return params




def locs_based_analysis(movie_path_list, ref_movie_path, pattern, box_size=2, gpu=True,
                        roi=None, ref_roi=None, save=True):

    ''' This function corrects the position of the movies in the movie_path_list based on the reference movie
    Then it calculates the overlap between the reference movie and the movies in the movie_path_list'''

    # ------- prepare the reference locs -------
    ref = MovieClass(ref_movie_path, ref_roi, frame_range=0)
    if gpu:
        ref.lq_gpu_fitting(box=5)
    else:
        ref.lq_cpu_fitting(box=5)

    ref.overlap_prevent(box_radius=box_size)
    _, ref_image = _render.render(ref.locs, ref.info)

    if save:
        save_locs(ref_movie_path.replace('.tif', '.hdf5'), ref.locs, ref.info)

    nuc_locs = {}
    for movie_path in movie_path_list:
        # ----------------------- drift correction ----------
        mov = MovieClass(movie_path, roi)
        if gpu:
            mov.lq_gpu_fitting(box=5)
        else:
            mov.lq_gpu_fitting(box=5)

        mov.drift_correction()
        mov.overlap_prevent(box_radius=box_size)

        nuc = re.search(pattern, os.path.basename(movie_path)).group(1)

        # ----------------------- channel alignment ----------
        _, mov_image = _render.render(mov.locs, mov.info)
        shift_y, shift_x = rcc([ref_image, mov_image])
        mov.locs.x -= shift_x[1]
        mov.locs.y -= shift_y[1]
        nuc_locs[nuc] = mov.locs[['x', 'y']].copy()

        if save:
            save_locs(movie_path.replace('.tif', '.hdf5'), mov.locs, mov.info)

    # ----------------------- neighbour counting ----------------------
    ref_locs = ref.locs[['x', 'y']].copy()
    total_params = []
    for nuc in nuc_locs.keys():
        param = neighbour_counting(ref_locs, nuc_locs[nuc], nuc)
        total_params.append(param)

    total_params = pd.concat(total_params, axis=1)
    total_params.to_csv(os.path.dirname(ref_movie_path) + '/neighbour_counting.csv', index=False)


    return




if __name__ == "__main__":
    ref = "H:\jagadish_data\Gap_T_8nt\GAP_T_8nt_comp_df10_GAP_T_Localization.tif"
    ref_roi = [0, 0, 684, 428]  # green channel # Note that two NIMs have different width!!!!

    mov_list = ["H:\jagadish_data\Gap_T_8nt\GAP_T_8nt_comp_df10_GAP_T_degen100nM_S3T300nM.tif",
                "H:\jagadish_data\Gap_T_8nt\GAP_T_8nt_comp_df10_GAP_T_degen100nM_S3G300nM.tif",
                "H:\jagadish_data\Gap_T_8nt\GAP_T_8nt_comp_df10_GAP_T_degen100nM_S3C300nM.tif",
                "H:\jagadish_data\Gap_T_8nt\GAP_T_8nt_comp_df10_GAP_T_degen100nM_S3A300nM.tif"]
    roi = [0, 428, 684, 856]  # red channel

    locs_based_analysis(mov_list, ref, pattern=r'S3([A-Z])300nM', roi=roi, ref_roi=ref_roi,
                        save=True, gpu=True)






